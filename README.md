Retrieval-Augmented Generation (RAG) API with Ollama, FAISS, and FastAPI
=======================================================================

This project implements a Retrieval-Augmented Generation (RAG) pipeline using local models via Ollama for embeddings and text generation, FAISS for vector search, and FastAPI to serve the API. You can load your documents (e.g., TXT files), embed them, index with FAISS, and then serve a context-aware QA assistant.

-----------------------------------------------------------------------
PROJECT STRUCTURE
-----------------------------------------------------------------------

main_ollama.py         # FastAPI app for RAG API
preprocess.py          # Preprocess and embed documents, build FAISS index
requirements.txt       # Python dependencies
faiss_index            # FAISS index generated by preprocess.py
documents/
    Onboarding_QA_Assistant_HR_Document.txt

-----------------------------------------------------------------------
INSTALLATION
-----------------------------------------------------------------------

1. Clone this repository and cd into the folder.
2. Install dependencies:
    pip install -r requirements.txt
3. Install and run Ollama locally:
   https://ollama.com/download
   Make sure Ollama is running before you start embedding or serving the API.
4. Pull required Ollama models:
    ollama pull nomic-embed-text:v1.5
    ollama pull qwen3:0.6b

-----------------------------------------------------------------------
PREPARING THE KNOWLEDGE BASE
-----------------------------------------------------------------------

1. Place all your .txt documents to be indexed inside the documents/ directory.
2. Run preprocessing to embed and index:
    python preprocess.py

- This will split all TXT files into overlapping chunks.
- For each chunk, an embedding is generated using Ollama’s nomic-embed-text:v1.5.
- All embeddings are indexed with FAISS (faiss_index file is created).
- Note: If you add or update documents, rerun this step!

-----------------------------------------------------------------------
RUNNING THE API
-----------------------------------------------------------------------

1. Start the FastAPI app:
    uvicorn main_ollama:app --reload

2. API documentation is auto-generated at:
   http://localhost:8000/docs

-----------------------------------------------------------------------
API USAGE EXAMPLE
-----------------------------------------------------------------------

POST /generate-story

Request JSON:
{
  "prompt": "What are the steps involved in onboarding a new employee?",
  "max_tokens": 300
}

Response:
{
  "response": "To onboard a new employee, follow these steps: ..."
}

- The API finds the most relevant document chunks using FAISS vector search.
- The selected context is used as a knowledge base for the generation model (qwen3:0.6b) to answer your prompt.

-----------------------------------------------------------------------
SCRIPT EXPLANATIONS
-----------------------------------------------------------------------

preprocess.py
-------------
- Extracts all text from TXT files in documents/
- Splits text into overlapping chunks (default: 500 chars, 50 overlap)
- Embeds each chunk using Ollama’s embedding model.
- Creates & saves FAISS index for fast vector search.

main_ollama.py
--------------
- Loads the FAISS index and preprocessed text chunks.
- Defines a FastAPI endpoint to accept a prompt, retrieve relevant chunks, and generate a context-aware answer using Ollama’s LLM.

-----------------------------------------------------------------------
ADDING NEW KNOWLEDGE
-----------------------------------------------------------------------

1. Drop new .txt files in documents/
2. Re-run:
    python preprocess.py

This reindexes and updates faiss_index.

-----------------------------------------------------------------------
TROUBLESHOOTING
-----------------------------------------------------------------------

- Ollama not running? Start Ollama before running scripts.
- Missing embeddings or errors in preprocessing? Check Ollama model is pulled and available.
- FAISS dimension mismatch? Ensure all embeddings are from the same model and have the same vector size.

-----------------------------------------------------------------------
REQUIREMENTS.TXT SAMPLE
-----------------------------------------------------------------------

fastapi
uvicorn
requests
faiss-cpu
ollama
python-docx

-----------------------------------------------------------------------
NOTES
-----------------------------------------------------------------------

- All embeddings and generation happen locally; no cloud APIs needed.
- To add new document types (PDF, DOCX), extend preprocess.py.
- The project can be adapted for more advanced pipelines, including multiple retrieval or LLM backends.

-----------------------------------------------------------------------
LICENSE
-----------------------------------------------------------------------

MIT License (or specify your preferred license).

-----------------------------------------------------------------------
ACKNOWLEDGEMENTS
-----------------------------------------------------------------------

- Ollama for fast, local LLM/embedding inference.
- FAISS for efficient similarity search.
- LangChain for inspiration on chunking/splitting.

